{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2b1bce-e88a-4b01-9dff-ff3a2bbb4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import datetime \n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9ad3ef5-cffc-49f1-8c54-570eaacac608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from flask import Flask, request, jsonify\n",
    "import datetime as dt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b48719-cb37-4983-af2d-d966afcd2aaf",
   "metadata": {},
   "source": [
    "#### get words for all reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a9de94-7dbc-45e9-b0bb-88b8e3a8c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words():\n",
    "    tfidf_matrix = np.load('words.npy', allow_pickle=True)\n",
    "    all_words = tfidf_matrix.tolist()\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e4346-a2bd-4e9f-b348-0dd51f5586fa",
   "metadata": {},
   "source": [
    "#### download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7fe16d-c6c5-48f4-91ab-ca7eed84d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name):\n",
    "    with open(name, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe209978-585a-4789-b6b3-361e97373f7e",
   "metadata": {},
   "source": [
    "## get words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6bac0b-f628-4f50-8def-043b8d5f5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date\n",
    "def get_date(soup):\n",
    "    date_span = soup.find('span', class_='submitted')\n",
    "    date = date_span.find('span').get('content')\n",
    "    date_object = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S%z')\n",
    "    return date_object.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aff5664-d4a7-4f42-9e25-5f5000bfe530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title\n",
    "def get_title(soup):\n",
    "    title_h1 = soup.find('h1', class_='title')\n",
    "    return title_h1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36eb906-c093-4366-893f-d5c863509630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_html\n",
    "def get_html(soup):\n",
    "    html_element = soup.find('div', class_='field field-name-body field-type-text-with-summary field-label-hidden')\n",
    "    return html_element\n",
    "\n",
    "def get_html_text(soup):\n",
    "    html_element = get_html(soup)\n",
    "    html_main = html_element.decode() \n",
    "    return html_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08511bea-e84d-45dc-810b-61a78a3c0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_url\n",
    "def get_url(soup):\n",
    "    full_url_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "    if full_url_tag:\n",
    "        full_url = full_url_tag.get(\"href\")\n",
    "        return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94c51cd-794c-410b-8f17-89066b15705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text\n",
    "def get_text(soup):\n",
    "    html_element = get_html(soup)\n",
    "    html_div = html_element.find('div', class_='field-item even')\n",
    "    elements = html_div.find_all(['p', 'ul', 'ol', 'div'])\n",
    "    text = ''\n",
    "    for element in elements:\n",
    "        if element.name == 'div' and element.find('hr'):\n",
    "            break\n",
    "        text += element.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3ed40f6-b060-4e4b-8d7c-0f9a53488db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for dataframe\n",
    "def get_data(soup):\n",
    "    data = []\n",
    "    data.append(get_date(soup))\n",
    "    data.append(get_title(soup))\n",
    "    data.append(get_url(soup))\n",
    "    data.append(get_html_text(soup))\n",
    "    data.append(get_text(soup))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29fb6587-8959-4e54-814b-0d25a66b3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for request\n",
    "def build_url(days_to_substract):\n",
    "    today = datetime.datetime.today() - datetime.timedelta(days=days_to_substract)\n",
    "    month_name = calendar.month_name[today.month].lower()\n",
    "    url = f'https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-{month_name}-{today.day}-{today.year}'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfeafef3-9d0b-45f1-aeca-4e29c3adbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "def get_dataframe():\n",
    "    df = pd.DataFrame(columns=['date', 'title', 'full_url', 'main_html', 'main_text'])\n",
    "    days_to_substract = 0\n",
    "    while True:\n",
    "        days_to_substract += 1\n",
    "        url = build_url(days_to_substract)\n",
    "        answer = requests.get(url)\n",
    "        if not answer.status_code == 200: \n",
    "            continue\n",
    "        html_text = answer.text     \n",
    "        soup = BeautifulSoup(html_text, 'lxml')\n",
    "        df.loc[0] = get_data(soup)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab863de4-cd57-40b8-b39b-cb5859182e5a",
   "metadata": {},
   "source": [
    "### lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e89d075-dd53-4810-9f7f-aa2569467228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d457c8-fdc4-4a0f-ab6d-b8feb97329c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(report):\n",
    "    report = report.lower()\n",
    "    text = re.sub(r'\\d+', '', report)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in pos_tags:\n",
    "        wordnet_tag = get_wordnet_pos(tag)\n",
    "        if wordnet_tag is None:\n",
    "            lemmatized_tokens.append(token)\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(str(token), pos=wordnet_tag))\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eefb10-e7d7-4baf-9a60-695fa607ed8b",
   "metadata": {},
   "source": [
    "### transform words to matrix (medhod#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493cc150-ba63-45e8-b1ee-7523bb998edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_words():\n",
    "    all_words = get_all_words()\n",
    "    data_frame = get_dataframe()\n",
    "    data_frame['lemmatized_text'] = data_frame['main_text'].apply(lemmatize)\n",
    "    \n",
    "    lem_text = data_frame['lemmatized_text']\n",
    "    \n",
    "    lem_text[0] = [word for word in lem_text[0] if word in all_words]\n",
    "    data_frame['lemmatized_text'] = lem_text\n",
    "    df_copy = data_frame.copy()\n",
    "    df_copy.loc[len(data_frame)] = [None, None, None, None, None, all_words]\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ff9ed72-abb2-444e-9a83-aabd7c242784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_vect():\n",
    "    df_isw = get_df_with_words()\n",
    "    lemmatized_text_str = df_isw['lemmatized_text'].apply(lambda x: ' '.join(x))\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(lemmatized_text_str)\n",
    "    first_row = tfidf_matrix.getrow(0)\n",
    "    return first_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c971ab-e55f-400c-8d61-142932ca934c",
   "metadata": {},
   "source": [
    "## get weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f1491b-16f3-4b54-98e0-b271e313c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_response():\n",
    "    url_base_url = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"\n",
    "    RSA_KEY = \"\"\n",
    "    API_TOKEN = \"\"\n",
    "    location = \"Kyiv\"\n",
    "    date = \"next24hours\"\n",
    "    url_api_version = \"v1\"\n",
    "    url = f\"{url_base_url}/{location}/{date}?unitGroup=metric&include=hours&key={RSA_KEY}&contentType=json\"\n",
    "    headers = {\n",
    "      'X-Api-Key': API_TOKEN\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == requests.codes.ok:\n",
    "        return response\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798fb6d9-6f70-4043-9095-3811cc5e0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_pred():\n",
    "    location = \"Kyiv\"\n",
    "    response = get_weather_response()\n",
    "    response_json = json.loads(response.text)\n",
    "    \n",
    "    curr_hour = dt.datetime.now().hour + 1\n",
    "    next_hour = 0\n",
    "    \n",
    "    result = {}\n",
    "    if curr_hour > 11:\n",
    "        while curr_hour != 24:\n",
    "            first_res = {\n",
    "                'day_tempmax': response_json['days'][0]['tempmax'],\n",
    "                'day_tempmin': response_json['days'][0]['tempmin'],\n",
    "                'day_temp': response_json['days'][0]['temp'],\n",
    "                'day_dew': response_json['days'][0]['dew'],\n",
    "                'day_humidity': response_json['days'][0]['humidity'],\n",
    "                'day_precip': response_json['days'][0]['precip'],\n",
    "                'day_precipcover': response_json['days'][0]['precipcover'],\n",
    "                'day_snow': response_json['days'][0]['snow'],\n",
    "                'day_windgust': response_json['days'][0]['windgust'],\n",
    "                'day_windspeed': response_json['days'][0]['windspeed'],\n",
    "                'day_winddir': response_json['days'][0]['winddir'],\n",
    "                'day_pressure': response_json['days'][0]['pressure'],\n",
    "                'day_cloudcover': response_json['days'][0]['cloudcover'],\n",
    "                'day_visibility': response_json['days'][0]['visibility'],\n",
    "                'day_solarradiation': response_json['days'][0]['solarradiation'],\n",
    "                'day_solarenergy': response_json['days'][0]['solarenergy'],\n",
    "                'day_uvindex': response_json['days'][0]['uvindex'],\n",
    "                'day_moonphase': response_json['days'][0]['moonphase'],\n",
    "                'hour_temp': response_json['days'][0]['hours'][curr_hour]['temp'],\n",
    "                'hour_humidity': response_json['days'][0]['hours'][curr_hour]['humidity'],\n",
    "                'hour_dew': response_json['days'][0]['hours'][curr_hour]['dew'],\n",
    "                'hour_precip': response_json['days'][0]['hours'][curr_hour]['precip'],\n",
    "                'hour_precipprob': response_json['days'][0]['hours'][curr_hour]['precipprob'],\n",
    "                'hour_snow': response_json['days'][0]['hours'][curr_hour]['snow'],\n",
    "                'hour_snowdepth': response_json['days'][0]['hours'][curr_hour]['snowdepth'],\n",
    "                'hour_windgust': response_json['days'][0]['hours'][curr_hour]['windgust'],\n",
    "                'hour_windspeed': response_json['days'][0]['hours'][curr_hour]['windspeed'],\n",
    "                'hour_winddir': response_json['days'][0]['hours'][curr_hour]['winddir'],\n",
    "                'hour_pressure': response_json['days'][0]['hours'][curr_hour]['pressure'],\n",
    "                'hour_visibility': response_json['days'][0]['hours'][curr_hour]['visibility'],\n",
    "                'hour_cloudcover': response_json['days'][0]['hours'][curr_hour]['cloudcover'],\n",
    "                'hour_solarradiation': response_json['days'][0]['hours'][curr_hour]['solarradiation'],\n",
    "                'hour_uvindex': response_json['days'][0]['hours'][curr_hour]['uvindex'],\n",
    "                'hour_severerisk': response_json['days'][0]['hours'][curr_hour]['severerisk'],\n",
    "                'day_of_week': dt.datetime.now().day % 7,\n",
    "                'date1': str(dt.datetime.today().date())\n",
    "            }\n",
    "            date = f\"{location}, {response_json['days'][0]['datetime']}, {response_json['days'][0]['hours'][curr_hour]['datetime']}\"\n",
    "            curr_hour += 1\n",
    "            result[date] = first_res\n",
    "        while len(result) != 12:\n",
    "            second_res = {\n",
    "                'day_tempmax': response_json['days'][1]['tempmax'],\n",
    "                'day_tempmin': response_json['days'][1]['tempmin'],\n",
    "                'day_temp': response_json['days'][1]['temp'],\n",
    "                'day_dew': response_json['days'][1]['dew'],\n",
    "                'day_humidity': response_json['days'][1]['humidity'],\n",
    "                'day_precip': response_json['days'][1]['precip'],\n",
    "                'day_precipcover': response_json['days'][1]['precipcover'],\n",
    "                'day_snow': response_json['days'][1]['snow'],\n",
    "                'day_windgust': response_json['days'][1]['windgust'],\n",
    "                'day_windspeed': response_json['days'][1]['windspeed'],\n",
    "                'day_winddir': response_json['days'][1]['winddir'],\n",
    "                'day_pressure': response_json['days'][1]['pressure'],\n",
    "                'day_cloudcover': response_json['days'][1]['cloudcover'],\n",
    "                'day_visibility': response_json['days'][1]['visibility'],\n",
    "                'day_solarradiation': response_json['days'][1]['solarradiation'],\n",
    "                'day_solarenergy': response_json['days'][1]['solarenergy'],\n",
    "                'day_uvindex': response_json['days'][1]['uvindex'],\n",
    "                'day_moonphase': response_json['days'][1]['moonphase'],\n",
    "                'hour_temp': response_json['days'][1]['hours'][next_hour]['temp'],\n",
    "                'hour_humidity': response_json['days'][1]['hours'][next_hour]['humidity'],\n",
    "                'hour_dew': response_json['days'][1]['hours'][next_hour]['dew'],\n",
    "                'hour_precip': response_json['days'][1]['hours'][next_hour]['precip'],\n",
    "                'hour_precipprob': response_json['days'][1]['hours'][next_hour]['precipprob'],\n",
    "                'hour_snow': response_json['days'][1]['hours'][next_hour]['snow'],\n",
    "                'hour_snowdepth': response_json['days'][1]['hours'][next_hour]['snowdepth'],\n",
    "                'hour_windgust': response_json['days'][1]['hours'][next_hour]['windgust'],\n",
    "                'hour_windspeed': response_json['days'][1]['hours'][next_hour]['windspeed'],\n",
    "                'hour_winddir': response_json['days'][1]['hours'][next_hour]['winddir'],\n",
    "                'hour_pressure': response_json['days'][1]['hours'][next_hour]['pressure'],\n",
    "                'hour_visibility': response_json['days'][1]['hours'][next_hour]['visibility'],\n",
    "                'hour_cloudcover': response_json['days'][1]['hours'][next_hour]['cloudcover'],\n",
    "                'hour_solarradiation': response_json['days'][1]['hours'][next_hour]['solarradiation'],\n",
    "                'hour_uvindex': response_json['days'][1]['hours'][next_hour]['uvindex'],\n",
    "                'hour_severerisk': response_json['days'][1]['hours'][next_hour]['severerisk'],\n",
    "                'day_of_week': (dt.datetime.today() + dt.timedelta(days=1)).day % 7,\n",
    "                'date1': str(dt.datetime.today().date())\n",
    "            }\n",
    "            date = f\"{location}, {response_json['days'][1]['datetime']}, {response_json['days'][1]['hours'][next_hour]['datetime']}\"\n",
    "            next_hour += 1\n",
    "            result[date] = second_res\n",
    "    else:\n",
    "        for i in range(12):\n",
    "            first_res = {\n",
    "                'day_tempmax': response_json['days'][0]['tempmax'],\n",
    "                'day_tempmin': response_json['days'][0]['tempmin'],\n",
    "                'day_temp': response_json['days'][0]['temp'],\n",
    "                'day_dew': response_json['days'][0]['dew'],\n",
    "                'day_humidity': response_json['days'][0]['humidity'],\n",
    "                'day_precip': response_json['days'][0]['precip'],\n",
    "                'day_precipcover': response_json['days'][0]['precipcover'],\n",
    "                'day_snow': response_json['days'][0]['snow'],\n",
    "                'day_windgust': response_json['days'][0]['windgust'],\n",
    "                'day_windspeed': response_json['days'][0]['windspeed'],\n",
    "                'day_winddir': response_json['days'][0]['winddir'],\n",
    "                'day_pressure': response_json['days'][0]['pressure'],\n",
    "                'day_cloudcover': response_json['days'][0]['cloudcover'],\n",
    "                'day_visibility': response_json['days'][0]['visibility'],\n",
    "                'day_solarradiation': response_json['days'][0]['solarradiation'],\n",
    "                'day_solarenergy': response_json['days'][0]['solarenergy'],\n",
    "                'day_uvindex': response_json['days'][0]['uvindex'],\n",
    "                'day_moonphase': response_json['days'][0]['moonphase'],\n",
    "                'hour_temp': response_json['days'][0]['hours'][curr_hour]['temp'],\n",
    "                'hour_humidity': response_json['days'][0]['hours'][curr_hour]['humidity'],\n",
    "                'hour_dew': response_json['days'][0]['hours'][curr_hour]['dew'],\n",
    "                'hour_precip': response_json['days'][0]['hours'][curr_hour]['precip'],\n",
    "                'hour_precipprob': response_json['days'][0]['hours'][curr_hour]['precipprob'],\n",
    "                'hour_snow': response_json['days'][0]['hours'][curr_hour]['snow'],\n",
    "                'hour_snowdepth': response_json['days'][0]['hours'][curr_hour]['snowdepth'],\n",
    "                'hour_windgust': response_json['days'][0]['hours'][curr_hour]['windgust'],\n",
    "                'hour_windspeed': response_json['days'][0]['hours'][curr_hour]['windspeed'],\n",
    "                'hour_winddir': response_json['days'][0]['hours'][curr_hour]['winddir'],\n",
    "                'hour_pressure': response_json['days'][0]['hours'][curr_hour]['pressure'],\n",
    "                'hour_visibility': response_json['days'][0]['hours'][curr_hour]['visibility'],\n",
    "                'hour_cloudcover': response_json['days'][0]['hours'][curr_hour]['cloudcover'],\n",
    "                'hour_solarradiation': response_json['days'][0]['hours'][curr_hour]['solarradiation'],\n",
    "                'hour_uvindex': response_json['days'][0]['hours'][curr_hour]['uvindex'],\n",
    "                'hour_severerisk': response_json['days'][0]['hours'][curr_hour]['severerisk'],\n",
    "                'day_of_week': dt.today().day(),\n",
    "                'date1': str(dt.datetime.today().date())\n",
    "            }\n",
    "            date = f\"{location}, {response_json['days'][0]['datetime']}, {response_json['days'][0]['hours'][curr_hour]['datetime']}\"\n",
    "            result[date] = first_res\n",
    "            curr_hour += 1\n",
    "    return result         \n",
    "    #print(json.dumps(result, indent = 4))\n",
    "    # for i in range(len(result)):\n",
    "    #    print(result[i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3a1e44c-917d-4658-99f3-6c58bf6f0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_matrix():\n",
    "    result = get_weather_pred()\n",
    "    data = json.dumps(result, indent = 4)\n",
    "    df = pd.read_json(data)\n",
    "    weather_df = df.T\n",
    "    weather_df.drop(columns=['date1'], inplace=True)\n",
    "    weather_df.fillna(0)\n",
    "    weather_df = weather_df.reset_index(drop=True)\n",
    "    # because sparce matrix cannot use just float value\n",
    "    weather_df = weather_df.astype('float64')\n",
    "\n",
    "    weather_columns = weather_df.columns.tolist()\n",
    "    weather_matrix = weather_df.to_numpy()\n",
    "    \n",
    "    sparse_weather = csr_matrix(weather_matrix)\n",
    "    return sparse_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdafb0-da2d-48c8-a8f5-0262efa34981",
   "metadata": {},
   "source": [
    "### get final matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c2ea25-57c7-4e1b-aac2-0483c844cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_final_matrix():\n",
    "    words_one_row = get_words_vect()\n",
    "    weather = get_weather_matrix()\n",
    "    words = vstack([words_one_row] * 12)\n",
    "    # merge weather matrix with words matrix\n",
    "    combine_matrix = sp.hstack((words, weather))\n",
    "    return combine_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce5cd9-1f22-42bf-addd-37a1e5ddc641",
   "metadata": {},
   "source": [
    "## get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dfc40f3-2320-48bb-b834-22b83dd5ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_pred(model):\n",
    "    combine_matrix = get_final_matrix()\n",
    "    y_pred = model.predict(combine_matrix)\n",
    "    y_pred_probab = model.predict_proba(combine_matrix)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['binary', 'float'])\n",
    "    hours = 12\n",
    "    for i in range(hours):\n",
    "        #print(\"Example {}: Prediction: {}, Probability: {}\".format(i+1, y_pred_5[i], y_pred_proba[i]))\n",
    "        df.loc[len(df.index)] = [y_pred[i], y_pred_probab[i]]\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d3b12-3877-408f-a420-f293c85d33b6",
   "metadata": {},
   "source": [
    "### receive info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb0d15-228f-4bf0-95c0-6406fd47ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "@app.route(\"/prediction\", methods=[\"POST\"])\n",
    "def get_data():\n",
    "    json_data = request.get_json()\n",
    "    location = json_data.got('region')\n",
    "    location = location.lower()\n",
    "    model = get_model(f'models/{oblast}.pkl')\n",
    "    \n",
    "    prediction = get_final_pred(model)\n",
    "    prediction['float'] = prediction['float'].apply(lambda x: x.tolist())\n",
    "    data_dict = prediction.to_dict(orient='records')\n",
    "    return jsonify(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894faf56-6a10-420f-bf66-a81c23fa52ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
